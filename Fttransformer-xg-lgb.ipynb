{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3887,"databundleVersionId":32350,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install rtdl_revisiting_models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T03:29:07.969699Z","iopub.execute_input":"2025-10-28T03:29:07.969939Z","iopub.status.idle":"2025-10-28T03:30:18.306279Z","shell.execute_reply.started":"2025-10-28T03:29:07.969916Z","shell.execute_reply":"2025-10-28T03:30:18.305194Z"}},"outputs":[{"name":"stdout","text":"Collecting rtdl_revisiting_models\n  Downloading rtdl_revisiting_models-0.0.2-py3-none-any.whl.metadata (888 bytes)\nRequirement already satisfied: torch<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from rtdl_revisiting_models) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=1.8->rtdl_revisiting_models)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.8->rtdl_revisiting_models) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.8->rtdl_revisiting_models) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.8->rtdl_revisiting_models) (3.0.2)\nDownloading rtdl_revisiting_models-0.0.2-py3-none-any.whl (12 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rtdl_revisiting_models\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rtdl_revisiting_models-0.0.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===============================================================\n# HIGGS BOSON FTTransformer + SupCon + LGBM + XGB + Stacking (Tier1 + Tier2)\n# - FTTransformer backbone + SupCon projection head (hard-negative top-k)\n# - Weighted Focal loss (uses physics sample Weight)\n# - 5-fold CV for NN, LGB, XGB -> OOF preds\n# - Meta-learner (LogisticRegression) on OOF predictions (stacking)\n# - AMS thresholding & Kaggle submission\n# ===============================================================\n\nimport os, math, zipfile\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom rtdl_revisiting_models import FTTransformer\n\n# ---------------- Settings ----------------\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSEED = 42\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n\nBATCH_SIZE = 512\nEPOCHS = 40\nLR = 7e-4                 # slightly lower LR\nWEIGHT_DECAY = 3e-5\nPROJ_DIM = 64\n\n# Tuned SupCon params (from suggestions)\nSUPCON_TEMPERATURE = 0.08\nHARD_NEG_TOPK = 30\nINITIAL_SUP_W, MAX_SUP_W = 0.12, 0.45\n\n# FT params (conservative)\nD_BLOCK = 128\nN_BLOCKS = 4\nATTN_HEADS = 8\nFFN_MULT = 4\n\n# CV / ensemble\nN_FOLDS = 5\n\n# Paths (Kaggle)\nzip_files = {\n    \"train\": \"/kaggle/input/higgs-boson/training.zip\",\n    \"test\": \"/kaggle/input/higgs-boson/test.zip\",\n    \"submission\": \"/kaggle/input/higgs-boson/random_submission.zip\"\n}\nextract_dir = \"/kaggle/working/higgs_data/\"\nos.makedirs(extract_dir, exist_ok=True)\nfor key, path in zip_files.items():\n    if os.path.exists(path):\n        with zipfile.ZipFile(path, \"r\") as z:\n            z.extractall(extract_dir)\n            print(f\"{key} unzipped.\")\n    else:\n        print(f\"{key} zip not found.\")\n\nTRAIN_CSV = os.path.join(extract_dir, \"training.csv\")\nTEST_CSV = os.path.join(extract_dir, \"test.csv\")\nOUT_SUB = \"/kaggle/working/submission.csv\"\n\n# ---------------- AMS metric ----------------\ndef ams_score(s, b):\n    b_reg = 10.0\n    rad = 2.0 * ((s + b + b_reg) * math.log(1.0 + s / (b + b_reg)) - s)\n    return math.sqrt(rad) if rad > 0 else 0.0\n\n# ---------------- Dataset ----------------\nclass HiggsDataset(Dataset):\n    def __init__(self, X, y=None, sample_weight=None):\n        self.X = X.astype(np.float32)\n        self.y = y\n        self.sample_weight = sample_weight\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        x = torch.tensor(self.X[idx], dtype=torch.float32)\n        if self.y is None:\n            return x, torch.tensor(-1, dtype=torch.long), torch.tensor(0.0, dtype=torch.float32)\n        y = torch.tensor(int(self.y[idx]), dtype=torch.long)\n        w = torch.tensor(float(self.sample_weight[idx]) if self.sample_weight is not None else 1.0, dtype=torch.float32)\n        return x, y, w\n\n# ---------------- Hard-negative SupCon Loss ----------------\nclass HardNegSupConLoss(nn.Module):\n    def __init__(self, temperature=0.08, top_k=30):\n        super().__init__()\n        self.temperature = temperature\n        self.top_k = top_k\n\n    def forward(self, features, labels):\n        device = features.device\n        labels = labels.contiguous().view(-1, 1)\n        batch_size = features.shape[0]\n\n        sim = torch.div(torch.matmul(features, features.T), self.temperature)\n\n        same_label = torch.eq(labels, labels.T).float().to(device)\n        diag = torch.eye(batch_size, device=device)\n        pos_mask = same_label - diag\n        neg_mask = 1.0 - same_label\n        pos_counts = pos_mask.sum(dim=1)\n\n        sim_max, _ = torch.max(sim, dim=1, keepdim=True)\n        sim_stable = sim - sim_max.detach()\n        exp_sim = torch.exp(sim_stable)\n\n        pos_exp = exp_sim * pos_mask\n\n        neg_sim_masked = sim.clone()\n        neg_sim_masked[neg_mask == 0] = -1e9\n        if batch_size > 1:\n            max_neg_per_row = neg_mask.sum(dim=1).max().int().item()\n            k = min(self.top_k, max_neg_per_row) if max_neg_per_row > 0 else 0\n        else:\n            k = 0\n\n        eps = 1e-12\n        if k == 0:\n            denom = pos_exp.sum(dim=1) + (exp_sim * (1.0 - diag - pos_mask)).sum(dim=1)\n            log_prob = torch.log((pos_exp.sum(dim=1) + eps) / (denom + eps))\n            valid = pos_counts > 0\n            if valid.sum() == 0:\n                return torch.tensor(0.0, device=device)\n            loss = -log_prob[valid.bool()].mean()\n            return loss\n\n        topk_vals, topk_idx = torch.topk(neg_sim_masked, k=k, dim=1)\n        selected_neg_mask = torch.zeros_like(neg_mask)\n        arange = torch.arange(batch_size, device=device).unsqueeze(1).expand(-1, k)\n        selected_neg_mask[arange.reshape(-1), topk_idx.reshape(-1)] = 1.0\n        selected_neg_mask = selected_neg_mask * neg_mask\n\n        selected_neg_exp = exp_sim * selected_neg_mask\n        sum_pos_exp = pos_exp.sum(dim=1)\n        sum_neg_exp = selected_neg_exp.sum(dim=1)\n\n        valid = (pos_counts > 0)\n        if valid.sum() == 0:\n            return torch.tensor(0.0, device=device)\n\n        log_prob = torch.log((sum_pos_exp + eps) / (sum_pos_exp + sum_neg_exp + eps))\n        loss = -log_prob[valid.bool()].mean()\n        return loss\n\n# ---------------- FTTransformer + SupCon wrapper ----------------\nclass FTTransformerSupCon(nn.Module):\n    def __init__(self, n_features, proj_dim=PROJ_DIM, d_block=D_BLOCK, n_blocks=N_BLOCKS):\n        super().__init__()\n        # instantiate according to rtdl_revisiting_models API used earlier\n        self.ft = FTTransformer(\n            n_cont_features=n_features,\n            cat_cardinalities=[],\n            d_block=d_block,\n            n_blocks=n_blocks,\n            attention_n_heads=ATTN_HEADS,\n            ffn_d_hidden_multiplier=FFN_MULT,\n            attention_dropout=0.2,\n            ffn_dropout=0.2,\n            residual_dropout=0.1,\n            d_out=2\n        )\n        # projection head: from model logits (2-d) -> proj_dim\n        self.proj_head = nn.Sequential(\n            nn.Linear(2, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, proj_dim)\n        )\n\n    def forward(self, x, return_emb=False):\n        # pass x_cat=None for purely numerical inputs\n        out = self.ft(x, x_cat=None)\n        if isinstance(out, tuple):\n            out = out[0]\n        emb = F.normalize(self.proj_head(out), dim=1)\n        if return_emb:\n            return out, emb\n        return out\n\n# ---------------- Weighted focal loss ----------------\nclass WeightedFocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, logits, targets, sample_weight=None):\n        # logits: [B, C], targets: [B] (int)\n        ce = F.cross_entropy(logits, targets, reduction='none')  # per-sample CE\n        p_t = torch.exp(-ce)\n        focal = ((1 - p_t) ** self.gamma) * ce\n        if sample_weight is not None:\n            # sample_weight shape [B]\n            focal = focal * sample_weight\n            denom = sample_weight.sum()\n            return focal.sum() / (denom + 1e-12)\n        return focal.mean()\n\n# ---------------- Data prep ----------------\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\n# replace -999 with nan and create missing flags\ntrain_df.replace(-999.0, np.nan, inplace=True)\ntest_df.replace(-999.0, np.nan, inplace=True)\n\nfor c in train_df.columns:\n    if c in ['EventId', 'Weight', 'Label']:\n        continue\n    if (train_df[c] == -999).any() or (test_df[c] == -999).any():\n        train_df[c + '_miss'] = (train_df[c].isna()).astype(int)\n        test_df[c + '_miss'] = (test_df[c].isna()).astype(int)\n\n# fill numeric nan with median of train\n# Select numeric columns but exclude 'Weight' (not present in test)\nnumeric_cols = [c for c in train_df.select_dtypes(include=np.number).columns if c != \"Weight\"]\n\ntrain_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n\n# Only use intersection of numeric cols that exist in test_df\nnum_cols_test = [c for c in numeric_cols if c in test_df.columns]\ntest_df[num_cols_test] = test_df[num_cols_test].fillna(train_df[num_cols_test].median())\n\n\n# basic physics features (keep your previous ones)\nif 'DER_mass_MMC' in train_df.columns and 'DER_mass_vis' in train_df.columns:\n    train_df['mass_ratio'] = train_df['DER_mass_MMC'] / (train_df['DER_mass_vis'] + 1e-6)\n    test_df['mass_ratio']  = test_df['DER_mass_MMC'] / (test_df['DER_mass_vis'] + 1e-6)\nif 'PRI_tau_pt' in train_df.columns and 'PRI_met' in train_df.columns:\n    train_df['pt_ratio'] = train_df['PRI_tau_pt'] / (train_df['PRI_met'] + 1e-6)\n    test_df['pt_ratio']  = test_df['PRI_tau_pt'] / (test_df['PRI_met'] + 1e-6)\n\n# label and weights\ny = (train_df['Label'] == 's').astype(int).values\nweights = train_df['Weight'].values\nevent_ids_test = test_df['EventId'].values if 'EventId' in test_df.columns else None\n\n# drop meta columns\ntrain_features = train_df.drop(columns=['EventId','Weight','Label'], errors='ignore')\ntest_features  = test_df.drop(columns=['EventId'], errors='ignore')\n\n# fill any remaining na and scale\ntrain_features = train_features.fillna(train_features.median())\ntest_features = test_features.fillna(train_features.median())\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features.values.astype(np.float32))\nX_test = scaler.transform(test_features.values.astype(np.float32))\n\n# ---------------- CV setup ----------------\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# containers for OOF and test preds\noof_nn = np.zeros(len(X), dtype=np.float32)\ntest_pred_nn = np.zeros(len(X_test), dtype=np.float32)\n\noof_lgb = np.zeros(len(X), dtype=np.float32)\ntest_pred_lgb_folds = []  # accumulate fold test preds, average later\n\noof_xgb = np.zeros(len(X), dtype=np.float32)\ntest_pred_xgb_folds = []\n\n# SupCon instance\nsupcon = HardNegSupConLoss(temperature=SUPCON_TEMPERATURE, top_k=HARD_NEG_TOPK)\n\n# ---------------- CV: train NN (FT+SupCon) and gather OOF preds ----------------\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n===== NN Fold {fold+1}/{N_FOLDS} =====\")\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n    w_tr, w_va = weights[tr_idx], weights[va_idx]\n\n    tr_ds = HiggsDataset(X_tr, y_tr, sample_weight=w_tr)\n    va_ds = HiggsDataset(X_va, y_va, sample_weight=w_va)\n    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n    va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = FTTransformerSupCon(X.shape[1], proj_dim=PROJ_DIM).to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    criterion_cls = WeightedFocalLoss(alpha=0.75, gamma=2.0)\n\n    best_auc = 0.0\n    best_state = None\n    best_val_pred = None\n\n    for epoch in range(EPOCHS):\n        model.train()\n        frac = epoch / max(1, EPOCHS - 1)\n        # dynamic supcon weight schedule\n        if frac <= 0.5:\n            sup_w = INITIAL_SUP_W + (MAX_SUP_W - INITIAL_SUP_W) * (frac / 0.5)\n        else:\n            sup_w = MAX_SUP_W * (1.0 - 0.5 * (frac - 0.5) / 0.5)\n        sup_w = float(sup_w)\n\n        train_losses = []\n        for xb, yb, wb in tqdm(tr_loader, desc=f\"Fold{fold+1} Epoch{epoch+1}\", leave=False):\n            xb = xb.to(DEVICE)\n            yb = yb.to(DEVICE)\n            wb = wb.to(DEVICE)\n\n            optimizer.zero_grad()\n            logits, emb = model(xb, return_emb=True)  # logits shape [B,2]\n            loss_cls = criterion_cls(logits, yb, sample_weight=wb)\n            loss_sup = supcon(emb, yb)\n            loss = loss_cls + sup_w * loss_sup\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        scheduler.step()\n\n        # validation\n        model.eval()\n        val_probs = []\n        with torch.no_grad():\n            for xb, yb, wb in va_loader:\n                xb = xb.to(DEVICE)\n                out = model(xb)\n                p = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n                val_probs.extend(p)\n        val_auc = roc_auc_score(y_va, val_probs)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | train_loss={np.mean(train_losses):.5f} | val_auc={val_auc:.5f} | sup_w={sup_w:.4f}\")\n        if val_auc > best_auc:\n            best_auc = val_auc\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            best_val_pred = np.array(val_probs)\n\n    # load best\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    model.eval()\n\n    # record OOF preds\n    oof_nn[va_idx] = best_val_pred\n\n    # test preds for this fold\n    preds_test_fold = []\n    test_loader = DataLoader(HiggsDataset(X_test, None, None), batch_size=1024, shuffle=False)\n    with torch.no_grad():\n        for xb, _, _ in test_loader:\n            xb = xb.to(DEVICE)\n            out = model(xb)\n            p = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n            preds_test_fold.extend(p)\n    test_pred_nn += np.array(preds_test_fold) / N_FOLDS\n\n    print(f\"Fold {fold+1} NN best AUC={best_auc:.5f}\")\n\n# ---------------- CV: train LightGBM OOF preds ----------------\nprint(\"\\n=== LightGBM OOF training ===\")\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n\n    lgbm = lgb.LGBMClassifier(\n        n_estimators=1200, learning_rate=0.01, max_depth=-1, num_leaves=64,\n        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=SEED\n    )\n    lgbm.fit(\n    X_tr, y_tr,\n    eval_set=[(X_va, y_va)],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50),\n        lgb.log_evaluation(100)\n    ]\n    )\n\n    oof_lgb[va_idx] = lgbm.predict_proba(X_va)[:, 1]\n    pred_test = lgbm.predict_proba(X_test)[:, 1]\n    test_pred_lgb_folds.append(pred_test)\n    print(f\"Fold {fold+1} LGB AUC={roc_auc_score(y_va, oof_lgb[va_idx]):.5f}\")\n\ntest_pred_lgb = np.mean(test_pred_lgb_folds, axis=0)\n\n# ---------------- CV: train XGBoost OOF preds ----------------\nprint(\"\\n=== XGBoost OOF training ===\")\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n\n    xgbm = xgb.XGBClassifier(\n        n_estimators=1200, learning_rate=0.01, max_depth=6,\n        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n        eval_metric='auc', tree_method='hist', random_state=SEED, use_label_encoder=False\n    )\n    xgbm.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], early_stopping_rounds=50, verbose=100)\n    oof_xgb[va_idx] = xgbm.predict_proba(X_va)[:, 1]\n    pred_test = xgbm.predict_proba(X_test)[:, 1]\n    test_pred_xgb_folds.append(pred_test)\n    print(f\"Fold {fold+1} XGB AUC={roc_auc_score(y_va, oof_xgb[va_idx]):.5f}\")\n\ntest_pred_xgb = np.mean(test_pred_xgb_folds, axis=0)\n\n# ---------------- OOF summary ----------------\nprint(\"\\nNN CV AUC:\", roc_auc_score(y, oof_nn))\nprint(\"LGB CV AUC:\", roc_auc_score(y, oof_lgb))\nprint(\"XGB CV AUC:\", roc_auc_score(y, oof_xgb))\n\n# ---------------- Tier-2: meta-learner stacking ----------------\nprint(\"\\n=== Training meta-learner (stacking) ===\")\nstack_oof = np.vstack([oof_nn, oof_lgb, oof_xgb]).T\nstack_test = np.vstack([test_pred_nn, test_pred_lgb, test_pred_xgb]).T\n\nmeta = LogisticRegression(max_iter=2000, solver='lbfgs')\nmeta.fit(stack_oof, y, sample_weight=weights)   # use physics weights in meta training\n\nfinal_test_pred = meta.predict_proba(stack_test)[:, 1]\n# also compute oof pred by meta for AMS tuning\noof_meta = meta.predict_proba(stack_oof)[:, 1]\nprint(\"Stacked (meta) CV AUC:\", roc_auc_score(y, oof_meta))\n\n# ---------------- AMS threshold search (on meta OOF) ----------------\nthr_range = np.linspace(0.01, 0.99, 99)\nbest_thr, best_ams = 0.5, -1.0\nfor t in thr_range:\n    s = weights[(y == 1) & (oof_meta > t)].sum()\n    b = weights[(y == 0) & (oof_meta > t)].sum()\n    score = ams_score(s, b)\n    if score > best_ams:\n        best_ams, best_thr = score, t\nprint(f\"Best AMS on stacked OOF = {best_ams:.3f} @ thr={best_thr:.4f}\")\n\n# ---------------- Submission ----------------\nprint(\"\\nWriting submission...\")\nrankorder = np.argsort(np.argsort(final_test_pred)) + 1\nclasses = np.where(final_test_pred > best_thr, 's', 'b')\nsub = pd.DataFrame({\n    \"EventId\": event_ids_test,\n    \"RankOrder\": rankorder,\n    \"Class\": classes\n})\nsub.to_csv(OUT_SUB, index=False)\nprint(\"Saved submission to:\", OUT_SUB)\nprint(\"Final stacked OOF AUC:\", roc_auc_score(y, oof_meta))\nprint(\"Final stacked OOF AMS:\", best_ams)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T03:30:18.307722Z","iopub.execute_input":"2025-10-28T03:30:18.308109Z","execution_failed":"2025-10-28T03:34:53.386Z"}},"outputs":[{"name":"stdout","text":"train unzipped.\ntest unzipped.\nsubmission unzipped.\n\n===== NN Fold 1/5 =====\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/40 | train_loss=0.01706 | val_auc=0.87292 | sup_w=0.1200\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/40 | train_loss=0.01818 | val_auc=0.88398 | sup_w=0.1369\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/40 | train_loss=0.02001 | val_auc=0.88827 | sup_w=0.1538\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/40 | train_loss=0.02185 | val_auc=0.89106 | sup_w=0.1708\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/40 | train_loss=0.02372 | val_auc=0.89244 | sup_w=0.1877\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/40 | train_loss=0.02558 | val_auc=0.89231 | sup_w=0.2046\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/40 | train_loss=0.02744 | val_auc=0.89624 | sup_w=0.2215\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/40 | train_loss=0.02931 | val_auc=0.89474 | sup_w=0.2385\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/40 | train_loss=0.03118 | val_auc=0.89206 | sup_w=0.2554\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/40 | train_loss=0.03305 | val_auc=0.89660 | sup_w=0.2723\n","output_type":"stream"},{"name":"stderr","text":"Fold1 Epoch11:  57%|█████▋    | 224/391 [00:13<00:09, 17.02it/s]","output_type":"stream"}],"execution_count":null}]}