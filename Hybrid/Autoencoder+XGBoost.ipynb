{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3887,"databundleVersionId":32350,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n# HIGGS BOSON — Autoencoder (PyTorch) + XGBoost (Classification)\n# ===============================================================\n\nimport os, math, zipfile\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport xgboost as xgb\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# ---------------- Settings ----------------\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSEED = 42\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n\nBATCH_SIZE = 512\nEPOCHS = 40                 # used for AE training (kept same as your original)\nLR = 7e-4\nWEIGHT_DECAY = 3e-5\nPROJ_DIM = 64               # autoencoder bottleneck dim\n\nN_FOLDS = 5\n\n# ---------------- Kaggle File Handling ----------------\nzip_files = {\n    \"train\": \"/kaggle/input/higgs-boson/training.zip\",\n    \"test\": \"/kaggle/input/higgs-boson/test.zip\",\n    \"submission\": \"/kaggle/input/higgs-boson/random_submission.zip\"\n}\nextract_dir = \"/kaggle/working/higgs_data/\"\nos.makedirs(extract_dir, exist_ok=True)\nfor key, path in zip_files.items():\n    if os.path.exists(path):\n        with zipfile.ZipFile(path, \"r\") as z:\n            z.extractall(extract_dir)\n        print(f\"{key} unzipped.\")\n    else:\n        print(f\"{key} zip not found at {path}\")\n\nTRAIN_CSV = os.path.join(extract_dir, \"training.csv\")\nTEST_CSV = os.path.join(extract_dir, \"test.csv\")\nOUT_SUB = \"/kaggle/working/submission.csv\"\n\n# ---------------- AMS Metric ----------------\ndef ams_score(s, b):\n    b_reg = 10.0\n    rad = 2.0 * ((s + b + b_reg) * math.log(1.0 + s / (b + b_reg)) - s)\n    return math.sqrt(rad) if rad > 0 else 0.0\n\n# ---------------- Dataset ----------------\nclass HiggsDataset(Dataset):\n    def __init__(self, X, y=None, sample_weight=None):\n        self.X = X.astype(np.float32)\n        self.y = y\n        self.sample_weight = sample_weight\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx):\n        x = torch.tensor(self.X[idx], dtype=torch.float32)\n        if self.y is None:\n            return x, torch.tensor(-1, dtype=torch.long), torch.tensor(0.0)\n        y = torch.tensor(int(self.y[idx]), dtype=torch.long)\n        w = torch.tensor(float(self.sample_weight[idx]) if self.sample_weight is not None else 1.0)\n        return x, y, w\n\n# ---------------- Autoencoder ----------------\nclass Autoencoder(nn.Module):\n    def __init__(self, n_features, bottleneck_dim=64, hidden_dims=[128, 64]):\n        super().__init__()\n        dims = [n_features] + hidden_dims + [bottleneck_dim]\n        enc_layers = []\n        for i in range(len(dims)-1):\n            enc_layers.append(nn.Linear(dims[i], dims[i+1]))\n            if i < len(dims)-2:\n                enc_layers.append(nn.ReLU())\n        self.encoder = nn.Sequential(*enc_layers)\n        # decoder symmetric\n        dec_dims = [bottleneck_dim] + list(reversed(hidden_dims)) + [n_features]\n        dec_layers = []\n        for i in range(len(dec_dims)-1):\n            dec_layers.append(nn.Linear(dec_dims[i], dec_dims[i+1]))\n            if i < len(dec_dims)-2:\n                dec_layers.append(nn.ReLU())\n        self.decoder = nn.Sequential(*dec_layers)\n    def forward(self, x):\n        z = self.encoder(x)\n        recon = self.decoder(z)\n        return recon, z\n    def encode(self, x):\n        return self.encoder(x)\n\n# ---------------- Weighted Focal Loss (kept for reference but not used) ----------------\nclass WeightedFocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    def forward(self, logits, targets, sample_weight=None):\n        ce = F.cross_entropy(logits, targets, reduction='none')\n        p_t = torch.exp(-ce)\n        focal = ((1 - p_t) ** self.gamma) * ce\n        if sample_weight is not None:\n            focal = focal * sample_weight\n        return focal.sum() / (sample_weight.sum() + 1e-12)\n\n# ---------------- Load Data ----------------\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\ntrain_df.replace(-999.0, np.nan, inplace=True)\ntest_df.replace(-999.0, np.nan, inplace=True)\n\nfor c in train_df.columns:\n    if c in ['EventId','Weight','Label']: continue\n    if train_df[c].isna().any():\n        train_df[c+'_miss'] = train_df[c].isna().astype(int)\n        test_df[c+'_miss'] = test_df[c].isna().astype(int)\n\nnumeric_cols = [c for c in train_df.select_dtypes(include=np.number).columns if c != \"Weight\"]\ntrain_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\nnum_cols_test = [c for c in numeric_cols if c in test_df.columns]\ntest_df[num_cols_test] = test_df[num_cols_test].fillna(train_df[num_cols_test].median())\n\nif {'DER_mass_MMC','DER_mass_vis'}.issubset(train_df.columns):\n    train_df['mass_ratio'] = train_df['DER_mass_MMC']/(train_df['DER_mass_vis']+1e-6)\n    test_df['mass_ratio'] = test_df['DER_mass_MMC']/(test_df['DER_mass_vis']+1e-6)\nif {'PRI_tau_pt','PRI_met'}.issubset(train_df.columns):\n    train_df['pt_ratio'] = train_df['PRI_tau_pt']/(train_df['PRI_met']+1e-6)\n    test_df['pt_ratio'] = test_df['PRI_tau_pt']/(test_df['PRI_met']+1e-6)\n\ny = (train_df['Label'] == 's').astype(int).values\nweights = train_df['Weight'].values\nevent_ids_test = test_df['EventId'].values\n\ntrain_features = train_df.drop(columns=['EventId','Weight','Label'], errors='ignore')\ntest_features = test_df.drop(columns=['EventId'], errors='ignore')\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features.values.astype(np.float32))\nX_test = scaler.transform(test_features.values.astype(np.float32))\n\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# ---------------- Training (AE per-fold + XGBoost) ----------------\noof_xgb = np.zeros(len(X))\ntest_pred_xgb = np.zeros(len(X_test))\n\ntsne_before_list, tsne_after_list, tsne_labels = [], [], []\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n===== Fold {fold+1}/{N_FOLDS} =====\")\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n    w_tr, w_va = weights[tr_idx], weights[va_idx]\n\n    # Dataloaders for AE training (unsupervised)\n    tr_loader = DataLoader(HiggsDataset(X_tr), batch_size=BATCH_SIZE, shuffle=True)\n    # instantiate AE\n    ae = Autoencoder(n_features=X.shape[1], bottleneck_dim=PROJ_DIM, hidden_dims=[128, 64]).to(DEVICE)\n\n    optimizer_ae = torch.optim.AdamW(ae.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler_ae = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ae, T_max=EPOCHS)\n\n    # Save \"before\" raw features for t-SNE (we'll use the raw val features as 'before')\n    tsne_before_list.append(X_va.copy())\n    tsne_labels.append(y_va)\n\n    # AE training\n    ae.train()\n    for epoch in range(EPOCHS):\n        epoch_loss = 0.0\n        for xb, _, _ in tr_loader:\n            xb = xb.to(DEVICE)\n            optimizer_ae.zero_grad()\n            recon, _ = ae(xb)\n            loss = F.mse_loss(recon, xb)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(ae.parameters(), 5.0)\n            optimizer_ae.step()\n            epoch_loss += loss.item() * xb.size(0)\n        scheduler_ae.step()\n        # optional: print small progress\n        if (epoch+1) % 10 == 0 or epoch==0:\n            print(f\"  AE Epoch {epoch+1}/{EPOCHS} — loss: {epoch_loss / len(X_tr):.6f}\")\n\n    ae.eval()\n    # encode train/val/test to get bottleneck features\n    with torch.no_grad():\n        X_tr_lat = []\n        for i in range(0, len(X_tr), BATCH_SIZE):\n            xb = torch.tensor(X_tr[i:i+BATCH_SIZE], dtype=torch.float32).to(DEVICE)\n            z = ae.encode(xb)\n            X_tr_lat.append(z.cpu().numpy())\n        X_tr_lat = np.vstack(X_tr_lat)\n\n        X_va_lat = []\n        for i in range(0, len(X_va), BATCH_SIZE):\n            xb = torch.tensor(X_va[i:i+BATCH_SIZE], dtype=torch.float32).to(DEVICE)\n            z = ae.encode(xb)\n            X_va_lat.append(z.cpu().numpy())\n        X_va_lat = np.vstack(X_va_lat)\n\n        X_test_lat = []\n        for i in range(0, len(X_test), BATCH_SIZE):\n            xb = torch.tensor(X_test[i:i+BATCH_SIZE], dtype=torch.float32).to(DEVICE)\n            z = ae.encode(xb)\n            X_test_lat.append(z.cpu().numpy())\n        X_test_lat = np.vstack(X_test_lat)\n\n    tsne_after_list.append(X_va_lat.copy())\n\n    # ---------------- XGBoost classifier on latent features ----------------\n    # XGBoost parameters (reasonable defaults; adjust if needed)\n    xgb_clf = xgb.XGBClassifier(\n        n_estimators=2000,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        tree_method='auto',\n        use_label_encoder=False,\n        eval_metric='auc',\n        random_state=SEED\n    )\n\n    # Fit with early stopping on validation latent set\n    xgb_clf.fit(\n        X_tr_lat, y_tr,\n        sample_weight=w_tr,\n        eval_set=[(X_va_lat, y_va)],\n        sample_weight_eval_set=[w_va],\n        early_stopping_rounds=50,\n        verbose=False\n    )\n\n    # validation preds\n    val_probs = xgb_clf.predict_proba(X_va_lat)[:,1]\n    oof_xgb[va_idx] = val_probs\n\n    # test preds (average)\n    test_pred_xgb += xgb_clf.predict_proba(X_test_lat)[:,1] / N_FOLDS\n\n    val_auc = roc_auc_score(y_va, val_probs)\n    print(f\"Fold {fold+1} XGBoost val AUC: {val_auc:.5f}\")\n\n# ---------------- t-SNE Visualization (latent AFTER only) ----------------\nX_after = np.vstack(tsne_after_list)\ny_tsne = np.concatenate(tsne_labels)\nprint(\"Running t-SNE on validation latent embeddings (after AE training)...\")\ntsne = TSNE(n_components=2, perplexity=50, random_state=SEED, init='pca', n_iter=1000)\nX2d_after = tsne.fit_transform(X_after)\nplt.figure(figsize=(7,6))\nplt.scatter(X2d_after[:,0], X2d_after[:,1], c=y_tsne, s=6, alpha=0.7)\nplt.title(\"t-SNE of Validation Latent Embeddings — AFTER (Autoencoder)\")\nplt.xlabel(\"TSNE-1\"); plt.ylabel(\"TSNE-2\")\nplt.show()\n\n# ---------------- AMS Optimization ----------------\nthr_range = np.linspace(0.01,0.99,99)\nbest_thr, best_ams = 0.5, -1\nfor t in thr_range:\n    s = weights[(y==1) & (oof_xgb>t)].sum()\n    b = weights[(y==0) & (oof_xgb>t)].sum()\n    sc = ams_score(s,b)\n    if sc > best_ams:\n        best_ams, best_thr = sc, t\nprint(f\"Best AMS on OOF = {best_ams:.3f} @ thr={best_thr:.4f}\")\n\n# ---------------- Submission ----------------\nprint(\"\\nWriting submission...\")\nrankorder = np.argsort(np.argsort(test_pred_xgb)) + 1\nclasses = np.where(test_pred_xgb > best_thr, 's', 'b')\nsub = pd.DataFrame({\"EventId\": event_ids_test, \"RankOrder\": rankorder, \"Class\": classes})\nsub.to_csv(OUT_SUB, index=False)\nprint(\"Saved submission to:\", OUT_SUB)\nprint(\"Final AMS:\", best_ams)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}