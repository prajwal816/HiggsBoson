{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3887,"databundleVersionId":32350,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install rtdl_revisiting_models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:59:19.358059Z","iopub.execute_input":"2025-10-28T14:59:19.358601Z","iopub.status.idle":"2025-10-28T15:00:42.908742Z","shell.execute_reply.started":"2025-10-28T14:59:19.358576Z","shell.execute_reply":"2025-10-28T15:00:42.907859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pytorch_tabnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:00:42.910223Z","iopub.execute_input":"2025-10-28T15:00:42.910430Z","iopub.status.idle":"2025-10-28T15:00:46.559974Z","shell.execute_reply.started":"2025-10-28T15:00:42.910410Z","shell.execute_reply":"2025-10-28T15:00:46.559180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# HIGGS BOSON — FTTransformer + SupCon + LGBM + XGB + TabNet + Stacking\n# ===============================================================\n\nimport os, math, zipfile\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom rtdl_revisiting_models import FTTransformer\nfrom pytorch_tabnet.tab_model import TabNetClassifier  # ✅ Added TabNet\n\n# ---------------- Settings ----------------\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSEED = 42\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n\nBATCH_SIZE = 512\nEPOCHS = 40\nLR = 7e-4\nWEIGHT_DECAY = 3e-5\nPROJ_DIM = 64\n\nSUPCON_TEMPERATURE = 0.08\nHARD_NEG_TOPK = 30\nINITIAL_SUP_W, MAX_SUP_W = 0.12, 0.45\n\nD_BLOCK = 128\nN_BLOCKS = 4\nATTN_HEADS = 8\nFFN_MULT = 4\n\nN_FOLDS = 5\n\n# ---------------- Kaggle File Handling ----------------\nzip_files = {\n    \"train\": \"/kaggle/input/higgs-boson/training.zip\",\n    \"test\":  \"/kaggle/input/higgs-boson/test.zip\",\n    \"submission\": \"/kaggle/input/higgs-boson/random_submission.zip\"\n}\nextract_dir = \"/kaggle/working/higgs_data/\"\nos.makedirs(extract_dir, exist_ok=True)\nfor key, path in zip_files.items():\n    if os.path.exists(path):\n        with zipfile.ZipFile(path, \"r\") as z:\n            z.extractall(extract_dir)\n            print(f\"{key} unzipped.\")\n    else:\n        print(f\"{key} zip not found at {path}\")\n\nTRAIN_CSV = os.path.join(extract_dir, \"training.csv\")\nTEST_CSV  = os.path.join(extract_dir, \"test.csv\")\nOUT_SUB  = \"/kaggle/working/submission.csv\"\n\n# ---------------- AMS Metric ----------------\ndef ams_score(s, b):\n    b_reg = 10.0\n    rad = 2.0 * ((s + b + b_reg) * math.log(1.0 + s / (b + b_reg)) - s)\n    return math.sqrt(rad) if rad > 0 else 0.0\n\n# ---------------- Dataset ----------------\nclass HiggsDataset(Dataset):\n    def __init__(self, X, y=None, sample_weight=None):\n        self.X = X.astype(np.float32)\n        self.y = y\n        self.sample_weight = sample_weight\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx):\n        x = torch.tensor(self.X[idx], dtype=torch.float32)\n        if self.y is None:\n            return x, torch.tensor(-1, dtype=torch.long), torch.tensor(0.0)\n        y = torch.tensor(int(self.y[idx]), dtype=torch.long)\n        w = torch.tensor(float(self.sample_weight[idx]) if self.sample_weight is not None else 1.0)\n        return x, y, w\n\n# ---------------- SupCon Loss ----------------\nclass HardNegSupConLoss(nn.Module):\n    def __init__(self, temperature=0.08, top_k=30):\n        super().__init__()\n        self.temperature = temperature\n        self.top_k = top_k\n    def forward(self, features, labels):\n        device = features.device\n        labels = labels.contiguous().view(-1,1)\n        bs = features.shape[0]\n        sim = torch.div(torch.matmul(features, features.T), self.temperature)\n        same = torch.eq(labels, labels.T).float().to(device)\n        diag = torch.eye(bs, device=device)\n        pos_mask = same - diag\n        neg_mask = 1.0 - same\n        sim_max, _ = torch.max(sim, dim=1, keepdim=True)\n        sim_stable = sim - sim_max.detach()\n        exp_sim = torch.exp(sim_stable)\n        pos_exp = exp_sim * pos_mask\n        neg_sim_masked = sim.clone()\n        neg_sim_masked[neg_mask == 0] = -1e9\n        eps = 1e-12\n        k = min(self.top_k, max(1, int(neg_mask.sum(dim=1).max().item())))\n        if k == 0:\n            denom = pos_exp.sum(dim=1) + (exp_sim*(1.0 - diag - pos_mask)).sum(dim=1)\n            log_prob = torch.log((pos_exp.sum(dim=1)+eps)/(denom+eps))\n            valid = pos_mask.sum(dim=1)>0\n            return -log_prob[valid].mean()\n        topk_vals, topk_idx = torch.topk(neg_sim_masked, k=k, dim=1)\n        sel_mask = torch.zeros_like(neg_mask)\n        arange = torch.arange(bs, device=device).unsqueeze(1).expand(-1,k)\n        sel_mask[arange.reshape(-1), topk_idx.reshape(-1)] = 1.0\n        sel_mask = sel_mask * neg_mask\n        sum_pos = pos_exp.sum(dim=1)\n        sum_neg = (exp_sim * sel_mask).sum(dim=1)\n        log_prob = torch.log((sum_pos + eps)/(sum_pos + sum_neg + eps))\n        valid = pos_mask.sum(dim=1)>0\n        return -log_prob[valid].mean()\n\n# ---------------- FTTransformer + SupCon ----------------\nclass FTTransformerSupCon(nn.Module):\n    def __init__(self, n_features, proj_dim=64):\n        super().__init__()\n        self.ft = FTTransformer(\n            n_cont_features=n_features,\n            cat_cardinalities=[],\n            d_block=D_BLOCK,\n            n_blocks=N_BLOCKS,\n            attention_n_heads=ATTN_HEADS,\n            ffn_d_hidden_multiplier=FFN_MULT,\n            attention_dropout=0.2,\n            ffn_dropout=0.2,\n            residual_dropout=0.1,\n            d_out=2\n        )\n        self.proj_head = nn.Sequential(nn.Linear(2,128), nn.ReLU(), nn.Linear(128,proj_dim))\n    def forward(self, x, return_emb=False):\n        out = self.ft(x, x_cat=None)\n        if isinstance(out, tuple): out = out[0]\n        emb = F.normalize(self.proj_head(out), dim=1)\n        return (out, emb) if return_emb else out\n\n# ---------------- Weighted Focal Loss ----------------\nclass WeightedFocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    def forward(self, logits, targets, sample_weight=None):\n        ce = F.cross_entropy(logits, targets, reduction='none')\n        p_t = torch.exp(-ce)\n        focal = ((1 - p_t) ** self.gamma) * ce\n        if sample_weight is not None:\n            focal = focal * sample_weight\n            return focal.sum() / (sample_weight.sum() + 1e-12)\n        return focal.mean()\n\n# ---------------- Load Data ----------------\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df  = pd.read_csv(TEST_CSV)\n\ntrain_df.replace(-999.0, np.nan, inplace=True)\ntest_df.replace(-999.0, np.nan, inplace=True)\n\nfor c in train_df.columns:\n    if c in ['EventId','Weight','Label']: continue\n    if train_df[c].isna().any():\n        train_df[c+'_miss'] = train_df[c].isna().astype(int)\n        test_df[c+'_miss']  = test_df[c].isna().astype(int)\n\nnumeric_cols = [c for c in train_df.select_dtypes(include=np.number).columns if c != \"Weight\"]\ntrain_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\nnum_cols_test = [c for c in numeric_cols if c in test_df.columns]\ntest_df[num_cols_test] = test_df[num_cols_test].fillna(train_df[num_cols_test].median())\n\n# Add derived features\nif {'DER_mass_MMC','DER_mass_vis'}.issubset(train_df.columns):\n    train_df['mass_ratio'] = train_df['DER_mass_MMC']/(train_df['DER_mass_vis']+1e-6)\n    test_df['mass_ratio']  = test_df['DER_mass_MMC']/(test_df['DER_mass_vis']+1e-6)\nif {'PRI_tau_pt','PRI_met'}.issubset(train_df.columns):\n    train_df['pt_ratio'] = train_df['PRI_tau_pt']/(train_df['PRI_met']+1e-6)\n    test_df['pt_ratio']  = test_df['PRI_tau_pt']/(test_df['PRI_met']+1e-6)\n\ny = (train_df['Label'] == 's').astype(int).values\nweights = train_df['Weight'].values\nevent_ids_test = test_df['EventId'].values\n\ntrain_features = train_df.drop(columns=['EventId','Weight','Label'], errors='ignore')\ntest_features  = test_df.drop(columns=['EventId'], errors='ignore')\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features.values.astype(np.float32))\nX_test = scaler.transform(test_features.values.astype(np.float32))\n\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# ---------------- Containers ----------------\noof_nn, oof_lgb, oof_xgb, oof_tab = [np.zeros(len(X)) for _ in range(4)]\ntest_pred_nn = np.zeros(len(X_test))\ntest_pred_lgb_folds, test_pred_xgb_folds, test_pred_tab_folds = [], [], []\n\nsupcon = HardNegSupConLoss(temperature=SUPCON_TEMPERATURE, top_k=HARD_NEG_TOPK)\n\n# ---------------- 1. FTTransformer + SupCon ----------------\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n===== NN Fold {fold+1}/{N_FOLDS} =====\")\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n    w_tr, w_va = weights[tr_idx], weights[va_idx]\n    tr_loader = DataLoader(HiggsDataset(X_tr, y_tr, w_tr), batch_size=BATCH_SIZE, shuffle=True)\n    va_loader = DataLoader(HiggsDataset(X_va, y_va, w_va), batch_size=BATCH_SIZE, shuffle=False)\n    model = FTTransformerSupCon(X.shape[1], PROJ_DIM).to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    criterion_cls = WeightedFocalLoss(alpha=0.75, gamma=2.0)\n    best_auc, best_state = 0, None\n    for epoch in range(EPOCHS):\n        model.train()\n        frac = epoch/max(1,EPOCHS-1)\n        sup_w = INITIAL_SUP_W+(MAX_SUP_W-INITIAL_SUP_W)*frac\n        for xb,yb,wb in tqdm(tr_loader, desc=f\"Fold{fold+1} Epoch{epoch+1}\", leave=False):\n            xb,yb,wb=xb.to(DEVICE),yb.to(DEVICE),wb.to(DEVICE)\n            optimizer.zero_grad()\n            logits, emb = model(xb, return_emb=True)\n            loss = criterion_cls(logits, yb, wb) + sup_w * supcon(emb, yb)\n            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0); optimizer.step()\n        scheduler.step()\n        model.eval(); val_probs=[]\n        with torch.no_grad():\n            for xb,_,_ in va_loader:\n                p=F.softmax(model(xb.to(DEVICE)),dim=1)[:,1].cpu().numpy(); val_probs.extend(p)\n        val_auc=roc_auc_score(y_va,val_probs)\n        if val_auc>best_auc: best_auc,val_probs_state=val_auc,val_probs; best_state={k:v.cpu() for k,v in model.state_dict().items()}\n    model.load_state_dict(best_state)\n    oof_nn[va_idx]=val_probs_state\n    model.eval(); preds_test_fold=[]\n    test_loader=DataLoader(HiggsDataset(X_test),batch_size=1024,shuffle=False)\n    with torch.no_grad():\n        for xb,_,_ in test_loader:\n            p=F.softmax(model(xb.to(DEVICE)),dim=1)[:,1].cpu().numpy(); preds_test_fold.extend(p)\n    test_pred_nn+=np.array(preds_test_fold)/N_FOLDS\n\n# ---------------- 2. LightGBM ----------------\nprint(\"\\n=== LightGBM ===\")\nfor tr, va in kf.split(X, y):\n    lgbm = lgb.LGBMClassifier(n_estimators=1200, learning_rate=0.01, num_leaves=64,\n                              subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0)\n    lgbm.fit(X[tr], y[tr], eval_set=[(X[va], y[va])],\n             callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)])\n    oof_lgb[va]=lgbm.predict_proba(X[va])[:,1]; test_pred_lgb_folds.append(lgbm.predict_proba(X_test)[:,1])\ntest_pred_lgb=np.mean(test_pred_lgb_folds,axis=0)\n\n# ---------------- 3. XGBoost ----------------\nprint(\"\\n=== XGBoost ===\")\nfor tr, va in kf.split(X, y):\n    xgbm = xgb.XGBClassifier(n_estimators=1200, learning_rate=0.01, max_depth=6,\n                             subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n                             eval_metric='auc', tree_method='hist', use_label_encoder=False)\n    xgbm.fit(X[tr], y[tr], eval_set=[(X[va], y[va])], early_stopping_rounds=50, verbose=100)\n    oof_xgb[va]=xgbm.predict_proba(X[va])[:,1]; test_pred_xgb_folds.append(xgbm.predict_proba(X_test)[:,1])\ntest_pred_xgb=np.mean(test_pred_xgb_folds,axis=0)\n\n\n# ---------------- 4. TabNet ----------------\nprint(\"\\n=== TabNet ===\")\nfor tr, va in kf.split(X, y):\n    tabnet = TabNetClassifier(\n        n_d=16, n_a=16, n_steps=5, gamma=1.5,\n        lambda_sparse=1e-4,\n        optimizer_fn=torch.optim.Adam,\n        optimizer_params=dict(lr=1e-3),\n        mask_type='sparsemax',\n        seed=SEED,\n        verbose=10  # ✅ moved here instead of .fit()\n    )\n\n    tabnet.fit(\n        X_train=X[tr], y_train=y[tr],\n        eval_set=[(X[va], y[va])],\n        patience=50, max_epochs=200, batch_size=1024, virtual_batch_size=128,\n        eval_metric=['auc']\n    )\n\n    oof_tab[va] = tabnet.predict_proba(X[va])[:, 1]\n    test_pred_tab_folds.append(tabnet.predict_proba(X_test)[:, 1])\n\ntest_pred_tab = np.mean(test_pred_tab_folds, axis=0)\n\n\n# ---------------- Stacking ----------------\nprint(\"\\n=== Meta-learner stacking ===\")\nstack_oof=np.vstack([oof_nn,oof_lgb,oof_xgb,oof_tab]).T\nstack_test=np.vstack([test_pred_nn,test_pred_lgb,test_pred_xgb,test_pred_tab]).T\nmeta=LogisticRegression(max_iter=2000)\nmeta.fit(stack_oof, y, sample_weight=weights)\noof_meta=meta.predict_proba(stack_oof)[:,1]\nfinal_test_pred=meta.predict_proba(stack_test)[:,1]\nprint(\"Stacked CV AUC:\", roc_auc_score(y, oof_meta))\n\n# ---------------- AMS ----------------\nthr_range=np.linspace(0.01,0.99,99)\nbest_thr,best_ams=0.5,-1\nfor t in thr_range:\n    s=weights[(y==1)&(oof_meta>t)].sum()\n    b=weights[(y==0)&(oof_meta>t)].sum()\n    sc=ams_score(s,b)\n    if sc>best_ams: best_ams,best_thr=sc,t\nprint(f\"Best AMS on stacked OOF = {best_ams:.3f} @ thr={best_thr:.4f}\")\n\n# ---------------- Submission ----------------\nprint(\"\\nWriting submission...\")\nrankorder=np.argsort(np.argsort(final_test_pred))+1\nclasses=np.where(final_test_pred>best_thr,'s','b')\nsub=pd.DataFrame({\"EventId\":event_ids_test,\"RankOrder\":rankorder,\"Class\":classes})\nsub.to_csv(OUT_SUB,index=False)\nprint(\"Saved submission to:\",OUT_SUB)\nprint(\"Final Stacked AMS:\",best_ams)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:00:46.560961Z","iopub.execute_input":"2025-10-28T15:00:46.561183Z","execution_failed":"2025-10-28T15:02:14.462Z"}},"outputs":[],"execution_count":null}]}