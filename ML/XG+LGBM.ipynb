{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3887,"databundleVersionId":32350,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n# HIGGS BOSON — XGBoost + LightGBM (K-Fold) + AMS + t-SNE (before/after)\n# Replaces FTTransformer + SupCon training with gradient-boosted trees.\n# ===============================================================\n\nimport os, math, zipfile\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# ---------------- Settings ----------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nN_FOLDS = 5\nBATCH_SIZE = 512  # not used for trees but kept for parity\nEARLY_STOPPING_ROUNDS = 100\nXGB_NUM_BOOST_ROUND = 5000\nLGB_NUM_BOOST_ROUND = 5000\n\n# Kaggle file handling (paths same as original)\nzip_files = {\n    \"train\": \"/kaggle/input/higgs-boson/training.zip\",\n    \"test\": \"/kaggle/input/higgs-boson/test.zip\",\n    \"submission\": \"/kaggle/input/higgs-boson/random_submission.zip\"\n}\nextract_dir = \"/kaggle/working/higgs_data/\"\nos.makedirs(extract_dir, exist_ok=True)\nfor key, path in zip_files.items():\n    if os.path.exists(path):\n        with zipfile.ZipFile(path, \"r\") as z:\n            z.extractall(extract_dir)\n        print(f\"{key} unzipped.\")\n    else:\n        print(f\"{key} zip not found at {path}\")\n\nTRAIN_CSV = os.path.join(extract_dir, \"training.csv\")\nTEST_CSV = os.path.join(extract_dir, \"test.csv\")\nOUT_SUB = \"/kaggle/working/submission.csv\"\n\n# ---------------- AMS Metric ----------------\ndef ams_score(s, b):\n    b_reg = 10.0\n    rad = 2.0 * ((s + b + b_reg) * math.log(1.0 + s / (b + b_reg)) - s)\n    return math.sqrt(rad) if rad > 0 else 0.0\n\n# ---------------- Load Data & Basic Feature Engineering ----------------\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\ntrain_df.replace(-999.0, np.nan, inplace=True)\ntest_df.replace(-999.0, np.nan, inplace=True)\n\n# create missing flags as in original\nfor c in train_df.columns:\n    if c in ['EventId', 'Weight', 'Label']:\n        continue\n    if train_df[c].isna().any():\n        train_df[c + '_miss'] = train_df[c].isna().astype(int)\n        if c in test_df.columns:\n            test_df[c + '_miss'] = test_df[c].isna().astype(int)\n        else:\n            test_df[c + '_miss'] = 0\n\nnumeric_cols = [c for c in train_df.select_dtypes(include=np.number).columns if c != \"Weight\"]\n# fill with median from train\ntrain_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\nnum_cols_test = [c for c in numeric_cols if c in test_df.columns]\ntest_df[num_cols_test] = test_df[num_cols_test].fillna(train_df[num_cols_test].median())\n\n# additional features mirroring original\nif {'DER_mass_MMC', 'DER_mass_vis'}.issubset(train_df.columns):\n    train_df['mass_ratio'] = train_df['DER_mass_MMC'] / (train_df['DER_mass_vis'] + 1e-6)\n    if 'DER_mass_MMC' in test_df.columns and 'DER_mass_vis' in test_df.columns:\n        test_df['mass_ratio'] = test_df['DER_mass_MMC'] / (test_df['DER_mass_vis'] + 1e-6)\n\nif {'PRI_tau_pt', 'PRI_met'}.issubset(train_df.columns):\n    train_df['pt_ratio'] = train_df['PRI_tau_pt'] / (train_df['PRI_met'] + 1e-6)\n    if 'PRI_tau_pt' in test_df.columns and 'PRI_met' in test_df.columns:\n        test_df['pt_ratio'] = test_df['PRI_tau_pt'] / (test_df['PRI_met'] + 1e-6)\n\n# labels, weights, event ids\ny = (train_df['Label'] == 's').astype(int).values\nweights = train_df['Weight'].values\nevent_ids_test = test_df['EventId'].values\n\n# drop IDs/target columns from features\ntrain_features = train_df.drop(columns=['EventId', 'Weight', 'Label'], errors='ignore')\ntest_features = test_df.drop(columns=['EventId'], errors='ignore')\n\n# keep consistent columns order\nfeats = [c for c in train_features.columns if c in test_features.columns or c in train_features.columns]\n# make sure same columns appear in test (some engineered could be missing)\ntest_features = test_features.reindex(columns=train_features.columns, fill_value=0)\n\n# scaling (not necessary for tree models but used for t-SNE stability)\nscaler = StandardScaler()\nX = scaler.fit_transform(train_features.values.astype(np.float32))\nX_test = scaler.transform(test_features.values.astype(np.float32))\n\n# cross-validation\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# ---------------- Storage for preds / cv ----------------\noof_xgb = np.zeros(len(X))\noof_lgb = np.zeros(len(X))\noof_ensemble = np.zeros(len(X))\ntest_pred_xgb = np.zeros(len(X_test))\ntest_pred_lgb = np.zeros(len(X_test))\ntest_pred_ensemble = np.zeros(len(X_test))\n\ntsne_embs_before = []\ntsne_embs_after = []\ntsne_labels = []\n\n# ---------------- Training per fold ----------------\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n===== Fold {fold+1}/{N_FOLDS} =====\")\n    X_tr, X_va = X[tr_idx], X[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n    w_tr, w_va = weights[tr_idx], weights[va_idx]\n\n    # t-SNE \"before\" on validation raw scaled features (for visualization)\n    tsne_embs_before.append(X_va.copy())\n    tsne_labels.append(y_va)\n\n    # ---------- XGBoost ----------\n    dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n    dvalid = xgb.DMatrix(X_va, label=y_va, weight=w_va)\n    dtest = xgb.DMatrix(X_test)\n\n    xgb_params = {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"verbosity\": 0,\n        \"seed\": SEED + fold,\n        \"tree_method\": \"hist\",\n        \"max_bin\": 256,\n        \"eta\": 0.05,\n        \"max_depth\": 6,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"lambda\": 1.0,\n        \"alpha\": 0.0,\n    }\n\n    xgb_watchlist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n    xgb_model = xgb.train(\n        params=xgb_params,\n        dtrain=dtrain,\n        num_boost_round=XGB_NUM_BOOST_ROUND,\n        evals=xgb_watchlist,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose_eval=100\n    )\n\n    pred_va_xgb = xgb_model.predict(dvalid, iteration_range=(0, xgb_model.best_iteration + 1))\n    pred_test_xgb = xgb_model.predict(dtest, iteration_range=(0, xgb_model.best_iteration + 1))\n\n    oof_xgb[va_idx] = pred_va_xgb\n    test_pred_xgb += pred_test_xgb / N_FOLDS\n\n    # ---------- LightGBM ----------\n    lgb_train = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n    lgb_valid = lgb.Dataset(X_va, label=y_va, weight=w_va, reference=lgb_train)\n\n    lgb_params = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"seed\": SEED + fold,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 63,\n        \"max_depth\": 10,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"lambda_l1\": 0.0,\n        \"lambda_l2\": 1.0,\n    }\n\n    lgb_model = lgb.train(\n        params=lgb_params,\n        train_set=lgb_train,\n        num_boost_round=LGB_NUM_BOOST_ROUND,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[\n            lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=True),\n            lgb.log_evaluation(100)\n        ]\n    )\n\n\n    pred_va_lgb = lgb_model.predict(X_va, num_iteration=lgb_model.best_iteration)\n    pred_test_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n\n    oof_lgb[va_idx] = pred_va_lgb\n    test_pred_lgb += pred_test_lgb / N_FOLDS\n\n    # ---------- Ensemble (simple average) ----------\n    pred_va_ens = 0.5 * pred_va_xgb + 0.5 * pred_va_lgb\n    pred_test_ens = 0.5 * pred_test_xgb + 0.5 * pred_test_lgb  # note: test_pred_xgb/lgb already being averaged across folds\n    oof_ensemble[va_idx] = pred_va_ens\n    # we accumulate test_pred_ensemble once per fold after we update test_pred_xgb/lgb at the top of loop\n    # To keep consistency, compute test fold contributions directly from this fold's predictions:\n    test_pred_ensemble += 0.5 * pred_test_xgb + 0.5 * pred_test_lgb\n\n    # ---------- diagnostics ----------\n    va_auc_xgb = roc_auc_score(y_va, pred_va_xgb)\n    va_auc_lgb = roc_auc_score(y_va, pred_va_lgb)\n    va_auc_ens = roc_auc_score(y_va, pred_va_ens)\n    print(f\"Fold {fold+1} AUCs -> XGB: {va_auc_xgb:.5f}, LGB: {va_auc_lgb:.5f}, ENS: {va_auc_ens:.5f}\")\n\n    # t-SNE \"after\" - augment validation features with ensemble probability as an extra dimension\n    X_va_after = np.hstack([X_va, pred_va_ens.reshape(-1, 1)])\n    tsne_embs_after.append(X_va_after)\n\n# finalize test ensemble averaging\n# test_pred_xgb and test_pred_lgb are already averaged across folds; test_pred_ensemble was accumulated fold-wise (we added per-fold predictions)\n# Normalize test_pred_ensemble (we added N_FOLDS times fold-specific half-averages):\ntest_pred_ensemble = test_pred_ensemble / N_FOLDS\n\n# ---------------- t-SNE Visualization ----------------\n# Concatenate per-fold arrays for visualization (before: raw features; after: features + pred)\nX_before_all = np.vstack(tsne_embs_before)\nX_after_all = np.vstack(tsne_embs_after)\ny_tsne = np.concatenate(tsne_labels)\n\nprint(\"Running t-SNE on 'before' (raw scaled validation features)...\")\ntsne = TSNE(n_components=2, perplexity=50, random_state=SEED, init='pca', n_iter=1000)\nX2d_before = tsne.fit_transform(X_before_all)\nplt.figure(figsize=(7,6))\nplt.scatter(X2d_before[:,0], X2d_before[:,1], c=y_tsne, s=6, alpha=0.7)\nplt.title(\"t-SNE of Validation Features — BEFORE\")\nplt.xlabel(\"TSNE-1\"); plt.ylabel(\"TSNE-2\")\nplt.show()\n\nprint(\"Running t-SNE on 'after' (validation features + ensemble prob)...\")\ntsne2 = TSNE(n_components=2, perplexity=50, random_state=SEED, init='pca', n_iter=1000)\nX2d_after = tsne2.fit_transform(X_after_all)\nplt.figure(figsize=(7,6))\nplt.scatter(X2d_after[:,0], X2d_after[:,1], c=y_tsne, s=6, alpha=0.7)\nplt.title(\"t-SNE of Validation Features + Ensemble Prob — AFTER\")\nplt.xlabel(\"TSNE-1\"); plt.ylabel(\"TSNE-2\")\nplt.show()\n\n# ---------------- OOF Metrics & AMS Optimization ----------------\n# Use the ensemble oof predictions for thresholding / AMS\noof_preds = oof_ensemble.copy()\noverall_auc = roc_auc_score(y, oof_preds)\nprint(f\"\\nOverall OOF AUC (ensemble) = {overall_auc:.5f}\")\n\nthr_range = np.linspace(0.01, 0.99, 99)\nbest_thr, best_ams = 0.5, -1\nfor t in thr_range:\n    s = weights[(y == 1) & (oof_preds > t)].sum()\n    b = weights[(y == 0) & (oof_preds > t)].sum()\n    sc = ams_score(s, b)\n    if sc > best_ams:\n        best_ams, best_thr = sc, t\nprint(f\"Best AMS on OOF = {best_ams:.3f} @ thr={best_thr:.4f}\")\n\n# ---------------- Prepare Submission ----------------\nprint(\"\\nWriting submission...\")\n# use the ensemble test predictions (we built test_pred_ensemble)\n# Ensure test_pred_ensemble length matches test set\nif len(test_pred_ensemble) != len(event_ids_test):\n    # as a fallback, average the per-model averaged predictions:\n    test_pred_ensemble = 0.5 * test_pred_xgb + 0.5 * test_pred_lgb\n\nrankorder = np.argsort(np.argsort(test_pred_ensemble)) + 1\nclasses = np.where(test_pred_ensemble > best_thr, 's', 'b')\nsub = pd.DataFrame({\"EventId\": event_ids_test, \"RankOrder\": rankorder, \"Class\": classes})\nsub.to_csv(OUT_SUB, index=False)\nprint(\"Saved submission to:\", OUT_SUB)\nprint(\"Final AMS:\", best_ams)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:52:02.661955Z","iopub.execute_input":"2025-11-01T12:52:02.662211Z","execution_failed":"2025-11-01T12:54:28.045Z"}},"outputs":[],"execution_count":null}]}